{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directoryA\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:17:26.765111Z","iopub.execute_input":"2025-05-05T16:17:26.765288Z","iopub.status.idle":"2025-05-05T16:17:28.468168Z","shell.execute_reply.started":"2025-05-05T16:17:26.765272Z","shell.execute_reply":"2025-05-05T16:17:28.467405Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/phones-data/cleaned_parallel_phones_val_phones.txt\n/kaggle/input/phones-data/cleaned_parallel_phones_test_phones.txt\n/kaggle/input/phones-data/cleaned_parallel_phones_test_hindi.txt\n/kaggle/input/phones-data/cleaned_parallel_phones_train_hindi.txt\n/kaggle/input/phones-data/cleaned_parallel_phones_val_hindi.txt\n/kaggle/input/phones-data/cleaned_parallel_phones_train_phones.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\ndef get_unique_source_tokens(filepaths):\n    \"\"\"Reads source files and returns unique whitespace-separated tokens.\"\"\"\n    all_tokens = Counter()\n    special_tokens = {\"<WB>\"} # Treat <WB> separately if desired\n    for filepath in filepaths:\n        if not os.path.exists(filepath):\n            print(f\"Warning: File not found - {filepath}\")\n            continue\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                # Split by whitespace, add individual characters/symbols if needed\n                tokens_in_line = line.strip().split()\n                all_tokens.update(tokens_in_line)\n\n    # Exclude special tokens if handled separately later,\n    # or keep them if you want to ensure they are added\n    unique_tokens = list(all_tokens.keys())\n    # Optionally filter out standard alphanumeric if not needed\n    # unique_tokens = [t for t in unique_tokens if not t.isalnum() and t not in special_tokens]\n    return unique_tokens\n\n# List your source data files\nsource_files = [\"/kaggle/input/phones-data/cleaned_parallel_phones_train_phones.txt\",\"/kaggle/input/phones-data/cleaned_parallel_phones_val_phones.txt\"] # Add test.src if you have one\nunique_phones = get_unique_source_tokens(source_files)\n\n# Optional: Print to review - you might want to refine this list\nprint(f\"Found {len(unique_phones)} unique tokens in source files.\")\nprint(unique_phones) # Uncomment to see the list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:17:32.551038Z","iopub.execute_input":"2025-05-05T16:17:32.551290Z","iopub.status.idle":"2025-05-05T16:17:32.712796Z","shell.execute_reply.started":"2025-05-05T16:17:32.551269Z","shell.execute_reply":"2025-05-05T16:17:32.712057Z"}},"outputs":[{"name":"stdout","text":"Found 62 unique tokens in source files.\n['ʋ', 'ɛ', 'd̪', 'ə', '<WB>', 'k', 'm', 'p', 'ɲ', 'i', 'z', 'f', 'n', 'dʒ', 'ɪ', 'ʃ', 'oː', 'eː', 'ɒː', 'ɒ', 'ʈ', 'l', 's', 'ɹ', 'c', 'bʲ', 'ʈʲ', 'tʃ', 'a', 'mʲ', 'ŋ', 'ʎ', 'ɖ', 'ɜː', 'fʲ', 'ɑː', 'b', 'iː', 'ɡ', 'j', 'ʉː', 'pʲ', 'aː', 'aj', 'ɛː', 'ɾ', 'ɑ', 'ɟʷ', 'aw', 'h', 'ʊ', 'ʉ', 'cʷ', 'ɟ', 'kʷ', 't̪', 'ʈʷ', 'ç', 'ʒ', 'ɔj', 'ɜ', 'w']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\n# Use the multilingual model checkpoint\nmodel_checkpoint = \"facebook/mbart-large-50-many-to-many-mmt\"\ntarget_lang_code = \"hi_IN\" # Hindi\nsource_lang_code = \"en_XX\" # Pragmatic choice for MBart\n\nprint(f\"Loading tokenizer: {model_checkpoint}\")\ntokenizer = MBart50TokenizerFast.from_pretrained(\n    model_checkpoint,\n    src_lang=source_lang_code,\n    tgt_lang=target_lang_code\n)\n\nprint(f\"Loading model: {model_checkpoint}\")\nmodel = MBartForConditionalGeneration.from_pretrained(model_checkpoint)\n\nprint(f\"Original tokenizer size: {len(tokenizer)}\")\nprint(f\"Original model embeddings size: {model.config.vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:17:33.372369Z","iopub.execute_input":"2025-05-05T16:17:33.373049Z","iopub.status.idle":"2025-05-05T16:18:11.258804Z","shell.execute_reply.started":"2025-05-05T16:17:33.373024Z","shell.execute_reply":"2025-05-05T16:18:11.258151Z"}},"outputs":[{"name":"stderr","text":"2025-05-05 16:17:45.991175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746461866.180889      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746461866.231945      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading tokenizer: facebook/mbart-large-50-many-to-many-mmt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da7d22225ef4b528b06ac9bc8631782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1c40be739e4b37b6f88a79869137a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80fa66f90e6a405286aba1b537fa372e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faf137f704d44cff8000f26977d9f141"}},"metadata":{}},{"name":"stdout","text":"Loading model: facebook/mbart-large-50-many-to-many-mmt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3310a82d3e0843f18353bc92642a92d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b2cbea272ff4053b1c6d1d415ce98a9"}},"metadata":{}},{"name":"stdout","text":"Original tokenizer size: 250054\nOriginal model embeddings size: 250054\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\ndef get_unique_source_tokens(filepaths):\n    \"\"\"Reads source files and returns unique whitespace-separated tokens.\"\"\"\n    all_tokens = Counter()\n    for filepath in filepaths:\n        if not os.path.exists(filepath):\n            print(f\"Warning: File not found - {filepath}\")\n            continue\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                tokens_in_line = line.strip().split()\n                all_tokens.update(tokens_in_line)\n    unique_tokens = list(all_tokens.keys())\n    return unique_tokens\n\n# List your source data files\nsource_files = [\"/kaggle/input/phones-data/cleaned_parallel_phones_train_phones.txt\",\"/kaggle/input/phones-data/cleaned_parallel_phones_val_phones.txt\"] # Add test.src if you have one\nunique_phones_to_add = get_unique_source_tokens(source_files)\n\nprint(f\"Found {len(unique_phones_to_add)} unique tokens in source files to potentially add.\")\n\n# Add ALL unique tokens found.\n# `add_tokens` will ignore any tokens already present in the vocabulary.\n# Setting special_tokens=False means they are treated as regular vocabulary items,\n# which is usually correct for phonetic symbols unless one needs very specific handling.\n# '<WB>' will be added as a regular token if it doesn't exist.\nif unique_phones_to_add:\n    print(f\"Attempting to add {len(unique_phones_to_add)} unique source tokens to the tokenizer...\")\n    num_added = tokenizer.add_tokens(unique_phones_to_add, special_tokens=False)\n    print(f\"Successfully added {num_added} new tokens (tokens already present are not re-added).\")\nelse:\n    print(\"No unique tokens found in source files to add.\")\n\nprint(f\"New tokenizer size: {len(tokenizer)}\")\n\n\n# --- CRITICAL: Resize Model Embeddings AFTER adding tokens ---\nprint(\"Resizing model embeddings...\")\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Verify the model's config reflects the new size\nprint(f\"New model embedding matrix size: {model.get_input_embeddings().weight.shape[0]}\")\n# It's good practice to assert this worked\nassert model.get_input_embeddings().weight.shape[0] == len(tokenizer), \"Model embedding size doesn't match new tokenizer size!\"\n\n# --- Rest of your script continues below (Data Loading, Preprocessing, Training Setup...) ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:18:11.259855Z","iopub.execute_input":"2025-05-05T16:18:11.260330Z","iopub.status.idle":"2025-05-05T16:18:17.532239Z","shell.execute_reply.started":"2025-05-05T16:18:11.260311Z","shell.execute_reply":"2025-05-05T16:18:17.531567Z"}},"outputs":[{"name":"stdout","text":"Found 62 unique tokens in source files to potentially add.\nAttempting to add 62 unique source tokens to the tokenizer...\nSuccessfully added 62 new tokens (tokens already present are not re-added).\nNew tokenizer size: 250079\nResizing model embeddings...\n","output_type":"stream"},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"New model embedding matrix size: 250079\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n!pip install datasets\nfrom datasets import load_dataset, DatasetDict, Dataset\n\n# --- Load Raw Text Data --- (Assuming files exist)\nprint(\"Loading raw text data...\")\ntrain_src_lines = [line.strip() for line in open(\"/kaggle/input/phones-data/cleaned_parallel_phones_train_phones.txt\", \"r\", encoding=\"utf-8\")]\ntrain_tgt_lines = [line.strip() for line in open(\"/kaggle/input/phones-data/cleaned_parallel_phones_train_hindi.txt\", \"r\", encoding=\"utf-8\")]\nval_src_lines = [line.strip() for line in open(\"/kaggle/input/phones-data/cleaned_parallel_phones_val_phones.txt\", \"r\", encoding=\"utf-8\")]\nval_tgt_lines = [line.strip() for line in open(\"/kaggle/input/phones-data/cleaned_parallel_phones_val_hindi.txt\", \"r\", encoding=\"utf-8\")]\ntrain_data = [{\"src\": s, \"tgt\": t} for s, t in zip(train_src_lines, train_tgt_lines) if s and t]\nnum_val_samples = 300  # Adjust this number as needed\nval_src_lines = val_src_lines[:num_val_samples]\nval_tgt_lines = val_tgt_lines[:num_val_samples]\n# =============================\nval_data = [{\"src\": s, \"tgt\": t} for s, t in zip(val_src_lines, val_tgt_lines) if s and t]\nraw_datasets = DatasetDict({\n    'train': Dataset.from_list(train_data),\n    'validation': Dataset.from_list(val_data)\n})\n\n# --- Tokenization Function ---\nmax_input_length = 128\nmax_target_length = 128\n\ndef preprocess_function(examples):\n    # Set tokenizer language context correctly\n    tokenizer.src_lang = source_lang_code # e.g., \"en_XX\"\n\n    model_inputs = tokenizer(\n        examples[\"src\"],\n        max_length=max_input_length,\n        truncation=True,\n        padding=False # Padding handled by DataCollator\n    )\n\n    # Setup the tokenizer for target language\n    # Correct way for MBart: Use 'as_target_tokenizer' context manager\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"tgt\"],\n            max_length=max_target_length,\n            truncation=True,\n            padding=False # Padding handled by DataCollator\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# --- Apply Tokenization ---\nprint(\"Tokenizing datasets with extended tokenizer...\")\ntokenized_datasets = raw_datasets.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names\n)\nprint(tokenized_datasets)\n# Check an example to see if phonetic tokens get correct IDs\nprint(\"Example tokenized input:\", tokenizer.convert_ids_to_tokens(tokenized_datasets['train'][0]['input_ids']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:18:17.535854Z","iopub.execute_input":"2025-05-05T16:18:17.536073Z","iopub.status.idle":"2025-05-05T16:18:32.825957Z","shell.execute_reply.started":"2025-05-05T16:18:17.536056Z","shell.execute_reply":"2025-05-05T16:18:32.824934Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\nLoading raw text data...\nTokenizing datasets with extended tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7337 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0410e56c42ca432eaf2ed9e348ecb1a0"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5eac2ce1d7c4efb9afd9fe3c4c35f9f"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 7337\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 300\n    })\n})\nExample tokenized input: ['en_XX', 'ʋ', '▁', 'ɛ', '▁', 'd̪', '▁', 'ə', '▁', '<WB>', '▁', 'd̪', '▁', 'ə', '▁', '<WB>', '▁', 'k', '▁', 'ə', '▁', 'm', '▁', 'p', '▁', 'ə', '▁', 'ɲ', '▁', 'i', '▁', 'z', '▁', '<WB>', '▁', 'f', '▁', 'ə', '▁', 'n', '▁', '<WB>', '▁', 'p', '▁', 'ə', '▁', 'dʒ', '▁', 'ɪ', '▁', 'ʃ', '▁', 'ə', '▁', 'n', '▁', '<WB>', '▁', 'ɪ', '▁', 'z', '▁', '<WB>', '▁', 'oː', '▁', 'k', '▁', 'eː', '▁', '<WB>', '▁', 'ɒː', '▁', '<WB>', '▁', 'n', '▁', 'ɒ', '▁', 'ʈ', '</s>']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\nimport torch\nimport numpy as np\n!pip install evaluate\nimport evaluate\n# --- compute_metrics function (use the corrected version from previous response) ---\nmetric = evaluate.load(\"bleu\")\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\"\"\"\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple): preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) # Use the extended tokenizer\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True) # Use the extended tokenizer\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    if \"score\" in result:\n        result[\"bleu\"] = round(result[\"score\"], 4)\n        result.pop(\"score\")\n    else: result[\"bleu\"] = 0.0\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = round(np.mean(prediction_lens), 4)\n    # print(f\"\\nRAW compute_metrics result: {result}\\n\") # Optional debug\n    return result\n# --- End compute_metrics ---\n\"\"\"\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Basic preprocessing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n\n    # Compute BLEU\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    bleu_score = result[\"bleu\"] if \"bleu\" in result else 0.0\n\n    # Output only BLEU and generation length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    return {\n        \"bleu\": round(bleu_score, 4),\n        \"gen_len\": round(np.mean(prediction_lens), 4)\n    }\n\n\n# --- Training Arguments (Adjust for Fine-tuning) ---\noutput_dir = \"phonetic-hi-mbart-finetuned\"\nper_device_batch_size = 8 # May need to adjust based on GPU memory (MBart-large is big)\nnum_train_epochs = 20       # Start with fewer epochs for fine-tuning (e.g., 3-10)\nlearning_rate = 3e-5     # Typically smaller LR for fine-tuning (e.g., 2e-5 to 5e-5)\nuse_fp16 = torch.cuda.is_available()\n\nargs = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    eval_strategy=\"epoch\",\n    learning_rate=learning_rate,\n    per_device_train_batch_size=per_device_batch_size,\n    per_device_eval_batch_size=per_device_batch_size,\n    weight_decay=0.01,\n    save_strategy=\"epoch\",\n    save_only_model=True,\n    save_total_limit=1,\n    num_train_epochs=num_train_epochs,\n    predict_with_generate=True,\n    logging_strategy=\"epoch\", # Log metrics after each epoch's eval\n    fp16=use_fp16,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bleu\",\n    greater_is_better=True,\n    # Necessary for MBart when fine-tuning on specific language pairs\n    generation_max_length=max_target_length, # Ensure generation matches max length used\n    # report_to=\"none\",\n)\n\n# --- Data Collator ---\n# Use the extended tokenizer here as well\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, # Pass the extended tokenizer\n    model=model,\n    padding=True\n)\n\n# --- Trainer Initialization ---\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer, # Pass the extended tokenizer\n    compute_metrics=compute_metrics\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:19:07.994476Z","iopub.execute_input":"2025-05-05T16:19:07.994821Z","iopub.status.idle":"2025-05-05T16:19:16.369819Z","shell.execute_reply.started":"2025-05-05T16:19:07.994795Z","shell.execute_reply":"2025-05-05T16:19:16.369005Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3312205103.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"5c556254e04a65a22bedb25b2faac3fc1b5b5272\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:19:16.371026Z","iopub.execute_input":"2025-05-05T16:19:16.371290Z","iopub.status.idle":"2025-05-05T16:19:22.399776Z","shell.execute_reply.started":"2025-05-05T16:19:16.371270Z","shell.execute_reply":"2025-05-05T16:19:22.398944Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaketvempaty\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# --- Start Training ---\nprint(\"Starting fine-tuning...\")\ntry:\n    train_result = trainer.train()\n\n    # --- Save Final Model & Metrics ---\n    print(\"Fine-tuning finished.\")\n    trainer.save_model() # Saves the best model based on eval_bleu\n    metrics = train_result.metricsc\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n    # CRITICAL: Save the extended tokenizer with the fine-tuned model\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Fine-tuned model and EXTENDED tokenizer saved to {output_dir}\")\n\nexcept Exception as e:\n    print(f\"An error occurred during training: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T16:19:22.400870Z","iopub.execute_input":"2025-05-05T16:19:22.401547Z","iopub.status.idle":"2025-05-05T19:47:30.515208Z","shell.execute_reply.started":"2025-05-05T16:19:22.401462Z","shell.execute_reply":"2025-05-05T19:47:30.514551Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"name":"stdout","text":"Starting fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250505_161922-yf869hpc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/saketvempaty/huggingface/runs/yf869hpc' target=\"_blank\">phonetic-hi-mbart-finetuned</a></strong> to <a href='https://wandb.ai/saketvempaty/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/saketvempaty/huggingface' target=\"_blank\">https://wandb.ai/saketvempaty/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/saketvempaty/huggingface/runs/yf869hpc' target=\"_blank\">https://wandb.ai/saketvempaty/huggingface/runs/yf869hpc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18360' max='18360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18360/18360 3:27:53, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>3.163500</td>\n      <td>2.549330</td>\n      <td>0.065900</td>\n      <td>36.746700</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.210800</td>\n      <td>2.295098</td>\n      <td>0.118300</td>\n      <td>37.716700</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.728100</td>\n      <td>2.248982</td>\n      <td>0.125800</td>\n      <td>28.210000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.350700</td>\n      <td>2.302347</td>\n      <td>0.144300</td>\n      <td>30.730000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.037600</td>\n      <td>2.406799</td>\n      <td>0.132300</td>\n      <td>26.853300</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.779300</td>\n      <td>2.529534</td>\n      <td>0.143000</td>\n      <td>30.970000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.576400</td>\n      <td>2.606610</td>\n      <td>0.147100</td>\n      <td>31.313300</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.413700</td>\n      <td>2.716183</td>\n      <td>0.139000</td>\n      <td>28.056700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.291100</td>\n      <td>2.788279</td>\n      <td>0.139000</td>\n      <td>29.736700</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.206900</td>\n      <td>2.863130</td>\n      <td>0.146400</td>\n      <td>28.930000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.149200</td>\n      <td>2.929709</td>\n      <td>0.150400</td>\n      <td>29.320000</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.109100</td>\n      <td>2.971626</td>\n      <td>0.151900</td>\n      <td>30.356700</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.081100</td>\n      <td>3.035738</td>\n      <td>0.156500</td>\n      <td>30.700000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.062000</td>\n      <td>3.085558</td>\n      <td>0.152700</td>\n      <td>30.656700</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.048100</td>\n      <td>3.127835</td>\n      <td>0.147300</td>\n      <td>27.503300</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.038100</td>\n      <td>3.157219</td>\n      <td>0.157400</td>\n      <td>29.136700</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.030400</td>\n      <td>3.173403</td>\n      <td>0.152400</td>\n      <td>28.110000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.024100</td>\n      <td>3.192366</td>\n      <td>0.154200</td>\n      <td>28.730000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.020400</td>\n      <td>3.217086</td>\n      <td>0.156200</td>\n      <td>29.070000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.018100</td>\n      <td>3.221505</td>\n      <td>0.150800</td>\n      <td>28.226700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning finished.\n***** train metrics *****\n  epoch                    =       20.0\n  total_flos               = 37017372GF\n  train_loss               =     0.6169\n  train_runtime            = 3:28:01.26\n  train_samples_per_second =     11.757\n  train_steps_per_second   =      1.471\nFine-tuned model and EXTENDED tokenizer saved to phonetic-hi-mbart-finetuned\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}