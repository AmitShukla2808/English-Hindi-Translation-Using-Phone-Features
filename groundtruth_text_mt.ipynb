{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:59:04.904220Z","iopub.execute_input":"2025-04-14T18:59:04.904477Z","iopub.status.idle":"2025-04-14T18:59:05.193855Z","shell.execute_reply.started":"2025-04-14T18:59:04.904457Z","shell.execute_reply":"2025-04-14T18:59:05.193185Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets sentencepiece sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:59:05.528250Z","iopub.execute_input":"2025-04-14T18:59:05.529037Z","iopub.status.idle":"2025-04-14T18:59:10.772282Z","shell.execute_reply.started":"2025-04-14T18:59:05.529006Z","shell.execute_reply":"2025-04-14T18:59:10.771429Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, fsspec, sacrebleu\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 portalocker-3.1.1 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load your dataset (adjust separator if needed)\ndf = pd.read_csv(\"/kaggle/input/dataset-ugce/UGCE_en2indic_10k.csv\")\n# Drop rows with missing translations\ndf = df[['text', 'hi_text']].dropna()\n\n# Rename columns for compatibility\ndf = df.rename(columns={'text': 'translation_en', 'hi_text': 'translation_hi'})\n\n# Convert to HuggingFace dataset\ndataset = Dataset.from_pandas(df)\n\n# Format as translation pair\ndataset = dataset.map(lambda x: {'translation': {'en': x['translation_en'], 'hi': x['translation_hi']}})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:59:10.773700Z","iopub.execute_input":"2025-04-14T18:59:10.773962Z","iopub.status.idle":"2025-04-14T18:59:14.054758Z","shell.execute_reply.started":"2025-04-14T18:59:10.773941Z","shell.execute_reply":"2025-04-14T18:59:14.053971Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3817a4f322744690b875756ac3772560"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:59:14.055455Z","iopub.execute_input":"2025-04-14T18:59:14.055658Z","iopub.status.idle":"2025-04-14T18:59:14.061113Z","shell.execute_reply.started":"2025-04-14T18:59:14.055641Z","shell.execute_reply":"2025-04-14T18:59:14.060434Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['translation_en', 'translation_hi', 'translation'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\nmodel_name = \"facebook/mbart-large-50-many-to-many-mmt\"\ntokenizer = MBart50TokenizerFast.from_pretrained(model_name)\nmodel = MBartForConditionalGeneration.from_pretrained(model_name)\n\n# Set tokenizer for English → Hindi\ntokenizer.src_lang = \"en_XX\"\ntokenizer.tgt_lang = \"hi_IN\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:59:14.063518Z","iopub.execute_input":"2025-04-14T18:59:14.064017Z","iopub.status.idle":"2025-04-14T18:59:53.680353Z","shell.execute_reply.started":"2025-04-14T18:59:14.063988Z","shell.execute_reply":"2025-04-14T18:59:53.679541Z"}},"outputs":[{"name":"stderr","text":"2025-04-14 18:59:28.101268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744657168.333857      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744657168.402404      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"061b1e66731c4a9ca359459ce3548834"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0cddd9accae4239a57e3b98ae91536e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74e5f508c9aa44fcacc05025c627f291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7eee2412fce41849dc49d9544167c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e188eac50bf4105af1a8c872276b39b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf9d2754a30e475f952681dee03b0f64"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Iterate over the list of dictionaries for the \"translation\" column\n    inputs = [trans[\"en\"] for trans in examples[\"translation\"]]\n    targets = [trans[\"hi\"] for trans in examples[\"translation\"]]\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    \n    # Tokenize the targets in the target context\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:59:53.681250Z","iopub.execute_input":"2025-04-14T18:59:53.681795Z","iopub.status.idle":"2025-04-14T18:59:56.744711Z","shell.execute_reply.started":"2025-04-14T18:59:53.681774Z","shell.execute_reply":"2025-04-14T18:59:56.743878Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"156f486cfd9e40db96e8030f7fa2d983"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T18:59:56.745539Z","iopub.execute_input":"2025-04-14T18:59:56.745829Z","iopub.status.idle":"2025-04-14T19:00:16.067312Z","shell.execute_reply.started":"2025-04-14T18:59:56.745785Z","shell.execute_reply":"2025-04-14T19:00:16.065975Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\nSuccessfully installed transformers-4.51.3\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"5c556254e04a65a22bedb25b2faac3fc1b5b5272\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:00:16.068615Z","iopub.execute_input":"2025-04-14T19:00:16.068915Z","iopub.status.idle":"2025-04-14T19:00:24.364540Z","shell.execute_reply.started":"2025-04-14T19:00:16.068888Z","shell.execute_reply":"2025-04-14T19:00:24.363778Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaketvempaty\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n!pip install evaluate\nimport evaluate\nfrom transformers import Seq2SeqTrainer\nimport numpy as np\n\n# Load the BLEU metric\nbleu_metric = evaluate.load(\"bleu\")\n\ndef compute_metrics(eval_preds):\n    predictions, labels = eval_preds\n\n    # Decode the predictions and labels\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Prepare references as a list of lists\n    references = [[label] for label in decoded_labels]\n\n    # Compute BLEU score\n    result = bleu_metric.compute(predictions=decoded_preds, references=references)\n\n    # Return the BLEU score\n    return {\"bleu\": result[\"bleu\"]}\n\n\nsplit_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Test (Eval) dataset size: {len(eval_dataset)}\")\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./mbart50-hi\",\n    eval_strategy=\"epoch\",         # Evaluate at the end of each epoch\n    save_strategy=\"epoch\",         # Save a checkpoint at the end of each epoch\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    weight_decay=0.01,\n    save_total_limit=1,            # Keep only the most recent checkpoint\n    num_train_epochs=5,\n    predict_with_generate=True,\n    fp16=True,\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\nfrom transformers import TrainerCallback\nimport random\n\nimport csv\nfrom datetime import datetime\n\nimport numpy as np\n\nclass InferenceCallback(TrainerCallback):\n    def __init__(self, tokenizer, model, eval_dataset, num_samples=10, output_file=\"inference_samples.csv\"):\n        self.tokenizer = tokenizer\n        self.model = model\n        self.eval_dataset = eval_dataset\n        self.num_samples = num_samples\n        self.output_file = output_file\n\n        # Set seed here so it always selects the same samples\n        #random.seed(42)\n        #self.sample_indices = random.sample(range(len(self.eval_dataset)), self.num_samples)\n        rng = np.random.default_rng(seed=42)\n        self.sample_indices = rng.choice(len(self.eval_dataset), size=self.num_samples, replace=False).tolist()\n        # Prepare consistent text samples\n        self.sample_texts = [self.eval_dataset[i]['translation']['en'] for i in self.sample_indices]\n        self.references = [self.eval_dataset[i]['translation']['hi'] for i in self.sample_indices]\n        self.outputs = []\n\n    def on_epoch_end(self, args, state, control, **kwargs):\n        inputs = self.tokenizer(self.sample_texts, padding=True, truncation=True, return_tensors=\"pt\").to(self.model.device)\n        generated_ids = self.model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n        predictions = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n\n        for src, ref, pred in zip(self.sample_texts, self.references, predictions):\n            self.outputs.append({\n                \"epoch\": state.epoch,\n                \"source\": src,\n                \"reference\": ref,\n                \"prediction\": pred\n            })\n\n        print(f\"\\n--- Inference After Epoch {state.epoch:.1f} ---\")\n        for src, pred, ref in zip(self.sample_texts, predictions, self.references):\n            print(f\"Source: {src}\")\n            print(f\"Predicted: {pred}\")\n            print(f\"Reference: {ref}\")\n            print(\"------\")\n        print(\"\\n\")\n\n        return control\n\n    def on_train_end(self, args, state, control, **kwargs):\n        import pandas as pd\n        df = pd.DataFrame(self.outputs)\n        df.to_csv(self.output_file, index=False)\n        print(f\"\\n✅ Inference samples saved to {self.output_file}\\n\")\n\n\n\n# Instantiate the custom inference callback\ninference_callback = InferenceCallback(\n    tokenizer=tokenizer,\n    model=model,\n    eval_dataset=eval_dataset,\n    num_samples=10  # You can adjust this to 3 or 4 samples\n)\n\n# Create the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,  # Using the separate test set (1% of original data)\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[inference_callback]  # Add the inference callback here\n)\n\n# Train the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T19:00:24.365482Z","iopub.execute_input":"2025-04-14T19:00:24.366038Z","iopub.status.idle":"2025-04-14T21:10:01.769364Z","shell.execute_reply.started":"2025-04-14T19:00:24.366019Z","shell.execute_reply":"2025-04-14T21:10:01.768593Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"198c1ba88094437490f9145508d10c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83de1073e27247abba50f313813397c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d208d67f7c8b4687b8fa8bb3d3fd6d77"}},"metadata":{}},{"name":"stdout","text":"Train dataset size: 9000\nTest (Eval) dataset size: 1000\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3152583719.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250414_190033-edegv86c</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/saketvempaty/huggingface/runs/edegv86c' target=\"_blank\">./mbart50-hi</a></strong> to <a href='https://wandb.ai/saketvempaty/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/saketvempaty/huggingface' target=\"_blank\">https://wandb.ai/saketvempaty/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/saketvempaty/huggingface/runs/edegv86c' target=\"_blank\">https://wandb.ai/saketvempaty/huggingface/runs/edegv86c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11250' max='11250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11250/11250 2:09:20, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.271600</td>\n      <td>0.272295</td>\n      <td>0.397197</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.187200</td>\n      <td>0.266218</td>\n      <td>0.414428</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.122000</td>\n      <td>0.271792</td>\n      <td>0.415650</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.079500</td>\n      <td>0.291028</td>\n      <td>0.409004</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.059700</td>\n      <td>0.299615</td>\n      <td>0.410370</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Inference After Epoch 1.0 ---\nSource: Suppose there are two partners X and Y, out of 70 say it is 1:1\nPredicted: मान लें कि दो भागीदार X और Y हैं, 70 में से यह 1:1 है\nReference: मान लीजिए कि दो साझेदार X और Y हैं, 70 में से यह 1: 1 है\n------\nSource: This stage uses information from the crime scene to determine both behavioral and personal characteristics and build an outline for the criminal profile\nPredicted: यह चरण अपराध स्थल से प्राप्त जानकारी का उपयोग व्यवहार संबंधी और व्यक्तिगत विशेषताओं को निर्धारित करने के लिए करता है और आपराधिक प्रोफाइल के लिए एक रूपरेखा बनाता है\nReference: यह चरण व्यवहार और व्यक्तिगत अभिलक्षणों दोनों को निर्धारित करने और आपराधिक प्रोफ़ाइल के लिए एक रूपरेखा बनाने के लिए अपराध स्थल से मिली जानकारी का उपयोग करता है\n------\nSource: They can assign the recovery material if necessary, define different paths for different learning goals\nPredicted: वे आवश्यक होने पर पुन:प्राप्ति सामग्री को आवंटित कर सकते हैं, विभिन्न शिक्षण लक्ष्यों के लिए विभिन्न मार्गों को परिभाषित कर सकते हैं\nReference: यदि आवश्यक हो, तो वे प्राप्त की गयी सामग्री को असाइनमेंट के रूप में दे सकते हैं, विभिन्न शैक्षिक लक्ष्यों हेतु अलग अलग कार्यप्रणाली निर्धारित कर सकते हैं\n------\nSource: Next limitation is pedagogical constraints\nPredicted: अगला सीमा शैक्षणिक बाधा है\nReference: अगली कमी शैक्षणिक बाधाओं से संबंधित है\n------\nSource: The DMCA amended title number 17 of the US Code to extend the reach of copyright, while limiting the liability of the providers of online services for copyright infringement by its users\nPredicted: डीएमसीए ने कॉपीराइट के दायरे को बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 को संशोधित किया, साथ ही अपने उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं की उत्तरदायित्व को सीमित किया\nReference: डीएमसीए ने अपने उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं के दायित्व को सीमित करते हुए कॉपीराइट की पहुंच बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया\n------\nSource: In case of public companies having shareholder base at the date of the meeting more than 5,000 members then the quorum shall constitute 30 members personally present at the meeting\nPredicted: सार्वजनिक कंपनियों के मामले में बैठक की तारीख पर 5,000 से अधिक सदस्यों के पास शेयरहोल्डर आधार होता है, तो बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन होगा\nReference: सार्वजनिक कंपनियों के 5000 से अधिक सदस्यों की बैठक की तारीख में शेयरधारक, आधार होने की स्थिति में, कोरम बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन करेगा\n------\nSource: Any authentication method is selected based on how it is used, how it is implemented, it's effectiveness, whether the user accepts and user's attitude, how far it is secured, installation expenses, etc\nPredicted: किसी भी ऑथेंटिकेशन विधि का चयन उस आधार पर किया जाता है कि इसका इस्तेमाल कैसे किया जाता है, यह कैसे लागू किया जाता है, यह प्रभावी है, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता के दृष्टिकोण, यह कितना सुरक्षित है, इंस्टॉलेशन व्यय, आदि\nReference: किसी भी प्रमाणीकरण विधि का चयन इस आधार पर किया जाता है कि इसका उपयोग कैसे किया जाता है, इसे कैसे कार्यान्वित किया जाता है, इसकी प्रभावशीलता क्या है, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता का दृष्टिकोण, यह कितना सुरक्षित है, स्थापना व्यय,आदि\n------\nSource: I can consider another important section that is section 35AD\nPredicted: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो धारा 35AD है\nReference: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो कि धारा 35AD है\n------\nSource: The main challenges that an OER faces today are quality issues since many OER repositories allow any user to create an account and post material, some resources may not be relevant and accurate\nPredicted: आज ओईआर का सामना करने वाली मुख्य चुनौतियां गुणवत्ता के मुद्दे हैं क्योंकि कई ओईआर रिपॉजिटरी किसी यूजर को एक खाता बनाने और सामग्री पोस्ट करने की अनुमति देते हैं, कुछ संसाधन प्रासंगिक और सटीक नहीं हो सकते हैं\nReference: आज ओईआर जिन चुनौतियों का सामना कर रहा है वे हैं गुणवत्ता के मुद्दे चूंकि कई ओईआर संग्राहक किसी भी उपयोगकर्ता को एक खाता खोलने और सामग्री पोस्ट करने की अनुमति देता है, तो कुछ संसाधन प्रासंगिक और एकदम सही नहीं हो सकते हैं\n------\nSource: If the problem it is silent, you can assume it but here it is specifically said that it is payable at the end of the year\nPredicted: यदि समस्या शांत है, तो आप इसे मान सकते हैं, लेकिन यहाँ विशेष रूप से कहा गया है कि यह वर्ष के अंत में देय है\nReference: और विशेष रूप से जानकारी यहाँ दी गई है\n------\n\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\n--- Inference After Epoch 2.0 ---\nSource: Suppose there are two partners X and Y, out of 70 say it is 1:1\nPredicted: मान लीजिए दो साझेदार X और Y हैं, 70 में से यह 1:1 है\nReference: मान लीजिए कि दो साझेदार X और Y हैं, 70 में से यह 1: 1 है\n------\nSource: This stage uses information from the crime scene to determine both behavioral and personal characteristics and build an outline for the criminal profile\nPredicted: यह मंच अपराध स्थल से प्राप्त जानकारी का उपयोग व्यवहार संबंधी और व्यक्तिगत विशेषताओं को निर्धारित करने और आपराधिक रूपरेखा के लिए एक रूपरेखा बनाने के लिए करता है\nReference: यह चरण व्यवहार और व्यक्तिगत अभिलक्षणों दोनों को निर्धारित करने और आपराधिक प्रोफ़ाइल के लिए एक रूपरेखा बनाने के लिए अपराध स्थल से मिली जानकारी का उपयोग करता है\n------\nSource: They can assign the recovery material if necessary, define different paths for different learning goals\nPredicted: वे आवश्यक स्थिति में पुनर्प्राप्ति सामग्री निर्धारित कर सकते हैं, विभिन्न सीखने के लक्ष्यों के लिए विभिन्न मार्गों को परिभाषित कर सकते हैं\nReference: यदि आवश्यक हो, तो वे प्राप्त की गयी सामग्री को असाइनमेंट के रूप में दे सकते हैं, विभिन्न शैक्षिक लक्ष्यों हेतु अलग अलग कार्यप्रणाली निर्धारित कर सकते हैं\n------\nSource: Next limitation is pedagogical constraints\nPredicted: अगली कमी शैक्षणिक बाधा है\nReference: अगली कमी शैक्षणिक बाधाओं से संबंधित है\n------\nSource: The DMCA amended title number 17 of the US Code to extend the reach of copyright, while limiting the liability of the providers of online services for copyright infringement by its users\nPredicted: डीएमसीए ने कॉपीराइट की पहुंच को बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया, जबकि इसके उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं की उत्तरदायित्व को सीमित किया\nReference: डीएमसीए ने अपने उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं के दायित्व को सीमित करते हुए कॉपीराइट की पहुंच बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया\n------\nSource: In case of public companies having shareholder base at the date of the meeting more than 5,000 members then the quorum shall constitute 30 members personally present at the meeting\nPredicted: यदि सार्वजनिक कंपनियों में शेयरहोल्डर के आधार पर बैठक की तारीख में,5,000 से अधिक सदस्य होते हैं, तो कोरम बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन करेगा\nReference: सार्वजनिक कंपनियों के 5000 से अधिक सदस्यों की बैठक की तारीख में शेयरधारक, आधार होने की स्थिति में, कोरम बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन करेगा\n------\nSource: Any authentication method is selected based on how it is used, how it is implemented, it's effectiveness, whether the user accepts and user's attitude, how far it is secured, installation expenses, etc\nPredicted: किसी भी ऑथेंटिकेशन विधि का चयन उस आधार पर किया जाता है कि इसे कैसे इस्तेमाल किया जाता है, इसे कैसे लागू किया जाता है, इसकी प्रभावशीलता, क्या यूजर स्वीकार करता है और यूजर का दृष्टिकोण, यह कितना सुरक्षित है, इंस्टॉलेशन व्यय, आदि\nReference: किसी भी प्रमाणीकरण विधि का चयन इस आधार पर किया जाता है कि इसका उपयोग कैसे किया जाता है, इसे कैसे कार्यान्वित किया जाता है, इसकी प्रभावशीलता क्या है, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता का दृष्टिकोण, यह कितना सुरक्षित है, स्थापना व्यय,आदि\n------\nSource: I can consider another important section that is section 35AD\nPredicted: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो धारा 35AD है\nReference: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो कि धारा 35AD है\n------\nSource: The main challenges that an OER faces today are quality issues since many OER repositories allow any user to create an account and post material, some resources may not be relevant and accurate\nPredicted: आज ओईआर के सामने आने वाली मुख्य चुनौतियां गुणवत्ता के मुद्दे हैं क्योंकि कई ओईआर रिपॉजिटरी किसी भी यूजर को एक खाता और पोस्ट मैटेरियल बनाने की अनुमति देते हैं, कुछ संसाधन प्रासंगिक और सटीक नहीं हो सकते हैं\nReference: आज ओईआर जिन चुनौतियों का सामना कर रहा है वे हैं गुणवत्ता के मुद्दे चूंकि कई ओईआर संग्राहक किसी भी उपयोगकर्ता को एक खाता खोलने और सामग्री पोस्ट करने की अनुमति देता है, तो कुछ संसाधन प्रासंगिक और एकदम सही नहीं हो सकते हैं\n------\nSource: If the problem it is silent, you can assume it but here it is specifically said that it is payable at the end of the year\nPredicted: यदि समस्या शांत है, तो आप इसे मान सकते हैं, लेकिन यहां विशेष रूप से कहा गया है कि यह वर्ष के अंत में देय है\nReference: और विशेष रूप से जानकारी यहाँ दी गई है\n------\n\n\n\n--- Inference After Epoch 3.0 ---\nSource: Suppose there are two partners X and Y, out of 70 say it is 1:1\nPredicted: मान लीजिए दो साझेदार हैं, X और Y, 70 में से यह 1:1 है\nReference: मान लीजिए कि दो साझेदार X और Y हैं, 70 में से यह 1: 1 है\n------\nSource: This stage uses information from the crime scene to determine both behavioral and personal characteristics and build an outline for the criminal profile\nPredicted: यह चरण व्यवहार संबंधी और व्यक्तिगत विशेषताओं को निर्धारित करने और आपराधिक प्रोफ़ाइल के लिए एक रूपरेखा बनाने के लिए अपराध दृश्य से प्राप्त जानकारी का उपयोग करता है\nReference: यह चरण व्यवहार और व्यक्तिगत अभिलक्षणों दोनों को निर्धारित करने और आपराधिक प्रोफ़ाइल के लिए एक रूपरेखा बनाने के लिए अपराध स्थल से मिली जानकारी का उपयोग करता है\n------\nSource: They can assign the recovery material if necessary, define different paths for different learning goals\nPredicted: वे आवश्यकता पड़ने पर पुनर्प्राप्ति सामग्री दे सकते हैं, विभिन्न शिक्षण लक्ष्यों के लिए विभिन्न मार्गों को परिभाषित कर सकते हैं\nReference: यदि आवश्यक हो, तो वे प्राप्त की गयी सामग्री को असाइनमेंट के रूप में दे सकते हैं, विभिन्न शैक्षिक लक्ष्यों हेतु अलग अलग कार्यप्रणाली निर्धारित कर सकते हैं\n------\nSource: Next limitation is pedagogical constraints\nPredicted: अगली कमी शैक्षणिक बाधा है\nReference: अगली कमी शैक्षणिक बाधाओं से संबंधित है\n------\nSource: The DMCA amended title number 17 of the US Code to extend the reach of copyright, while limiting the liability of the providers of online services for copyright infringement by its users\nPredicted: डीएमसीए ने अपने उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं की उत्तरदायित्व को सीमित करते हुए, कॉपीराइट की पहुंच को बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया\nReference: डीएमसीए ने अपने उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं के दायित्व को सीमित करते हुए कॉपीराइट की पहुंच बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया\n------\nSource: In case of public companies having shareholder base at the date of the meeting more than 5,000 members then the quorum shall constitute 30 members personally present at the meeting\nPredicted: सार्वजनिक कंपनियों के मामले में, जिनकी बैठक की तारीख में शेयरहोल्डर्स की आधार संख्या 5,000 से अधिक सदस्य होते हैं, तो कूरम व्यक्तिगत रूप से बैठक में उपस्थित 30 सदस्यों का गठन करेगा\nReference: सार्वजनिक कंपनियों के 5000 से अधिक सदस्यों की बैठक की तारीख में शेयरधारक, आधार होने की स्थिति में, कोरम बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन करेगा\n------\nSource: Any authentication method is selected based on how it is used, how it is implemented, it's effectiveness, whether the user accepts and user's attitude, how far it is secured, installation expenses, etc\nPredicted: किसी भी ऑथेंटिकेशन विधि का चयन उस आधार पर किया जाता है कि इसका उपयोग कैसे किया जाता है, इसे कैसे लागू किया जाता है, इसकी प्रभावशीलता, क्या यूजर स्वीकार करता है और यूजर का दृष्टिकोण, यह कितने सुरक्षित है, इंस्टॉलेशन खर्च, आदि\nReference: किसी भी प्रमाणीकरण विधि का चयन इस आधार पर किया जाता है कि इसका उपयोग कैसे किया जाता है, इसे कैसे कार्यान्वित किया जाता है, इसकी प्रभावशीलता क्या है, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता का दृष्टिकोण, यह कितना सुरक्षित है, स्थापना व्यय,आदि\n------\nSource: I can consider another important section that is section 35AD\nPredicted: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो धारा 35AD है\nReference: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो कि धारा 35AD है\n------\nSource: The main challenges that an OER faces today are quality issues since many OER repositories allow any user to create an account and post material, some resources may not be relevant and accurate\nPredicted: आज एक ओईआर के सामने आने वाली मुख्य चुनौतियां गुणवत्ता के मुद्दे हैं, क्योंकि कई ओईआर रिपॉजिटरीज़ किसी भी यूजर को खाते बनाने और पोस्ट मैटेरियल बनाने की अनुमति देते हैं, कुछ संसाधन प्रासंगिक और सटीक नहीं हो सकते हैं\nReference: आज ओईआर जिन चुनौतियों का सामना कर रहा है वे हैं गुणवत्ता के मुद्दे चूंकि कई ओईआर संग्राहक किसी भी उपयोगकर्ता को एक खाता खोलने और सामग्री पोस्ट करने की अनुमति देता है, तो कुछ संसाधन प्रासंगिक और एकदम सही नहीं हो सकते हैं\n------\nSource: If the problem it is silent, you can assume it but here it is specifically said that it is payable at the end of the year\nPredicted: अगर समस्या शांत है, आप इसे मान सकते हैं, लेकिन यहाँ यह विशेष रूप से कहा गया है कि यह वर्ष के अंत में देय है\nReference: और विशेष रूप से जानकारी यहाँ दी गई है\n------\n\n\n\n--- Inference After Epoch 4.0 ---\nSource: Suppose there are two partners X and Y, out of 70 say it is 1:1\nPredicted: मान लीजिए कि दो साझेदार हैं, X और Y, 70 में से यह 1:1 है\nReference: मान लीजिए कि दो साझेदार X और Y हैं, 70 में से यह 1: 1 है\n------\nSource: This stage uses information from the crime scene to determine both behavioral and personal characteristics and build an outline for the criminal profile\nPredicted: यह स्टेज अपराध स्थल से प्राप्त जानकारी का उपयोग व्यवहार संबंधी और व्यक्तिगत विशेषताओं को निर्धारित करने और आपराधिक प्रोफ़ाइल के लिए एक रूपरेखा बनाने के लिए करता है\nReference: यह चरण व्यवहार और व्यक्तिगत अभिलक्षणों दोनों को निर्धारित करने और आपराधिक प्रोफ़ाइल के लिए एक रूपरेखा बनाने के लिए अपराध स्थल से मिली जानकारी का उपयोग करता है\n------\nSource: They can assign the recovery material if necessary, define different paths for different learning goals\nPredicted: वे आवश्यकता पड़ने पर पुनर्प्राप्ति सामग्री दे सकते हैं, विभिन्न शिक्षण लक्ष्यों के लिए विभिन्न मार्गों को निर्धारित कर सकते हैं\nReference: यदि आवश्यक हो, तो वे प्राप्त की गयी सामग्री को असाइनमेंट के रूप में दे सकते हैं, विभिन्न शैक्षिक लक्ष्यों हेतु अलग अलग कार्यप्रणाली निर्धारित कर सकते हैं\n------\nSource: Next limitation is pedagogical constraints\nPredicted: अगली कमी शैक्षणिक बाधाएँ हैं\nReference: अगली कमी शैक्षणिक बाधाओं से संबंधित है\n------\nSource: The DMCA amended title number 17 of the US Code to extend the reach of copyright, while limiting the liability of the providers of online services for copyright infringement by its users\nPredicted: डीएमसीए ने कॉपीराइट की पहुंच को बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया, इसके उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं की लायबिलिटी को सीमित करते हुए\nReference: डीएमसीए ने अपने उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं के दायित्व को सीमित करते हुए कॉपीराइट की पहुंच बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया\n------\nSource: In case of public companies having shareholder base at the date of the meeting more than 5,000 members then the quorum shall constitute 30 members personally present at the meeting\nPredicted: सार्वजनिक कंपनियों के मामले में, जिसकी बैठक की तारीख में शेयरहोल्डर आधार पर 5,000 से अधिक सदस्य होते हैं, तो कोरम में व्यक्तिगत रूप से बैठक में उपस्थित 30 सदस्य शामिल होंगे\nReference: सार्वजनिक कंपनियों के 5000 से अधिक सदस्यों की बैठक की तारीख में शेयरधारक, आधार होने की स्थिति में, कोरम बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन करेगा\n------\nSource: Any authentication method is selected based on how it is used, how it is implemented, it's effectiveness, whether the user accepts and user's attitude, how far it is secured, installation expenses, etc\nPredicted: किसी भी ऑथेंटिकेशन विधि का चुनाव इस आधार पर किया जाता है कि इसका इस्तेमाल कैसे किया जाता है, यह कैसे लागू किया जाता है, इसकी प्रभावशीलता, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता का दृष्टिकोण, यह कितना सुरक्षित है, इंस्टॉलेशन व्यय आदि\nReference: किसी भी प्रमाणीकरण विधि का चयन इस आधार पर किया जाता है कि इसका उपयोग कैसे किया जाता है, इसे कैसे कार्यान्वित किया जाता है, इसकी प्रभावशीलता क्या है, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता का दृष्टिकोण, यह कितना सुरक्षित है, स्थापना व्यय,आदि\n------\nSource: I can consider another important section that is section 35AD\nPredicted: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूं, वह धारा 35AD है\nReference: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो कि धारा 35AD है\n------\nSource: The main challenges that an OER faces today are quality issues since many OER repositories allow any user to create an account and post material, some resources may not be relevant and accurate\nPredicted: आज एक ओईआर को जिन मुख्य चुनौतियों का सामना करना पड़ता है, वह गुणवत्ता के मुद्दे हैं क्योंकि कई ओईआर रिपॉजिटरीज़ किसी भी यूजर को खाते बनाने और पोस्ट मैटिरियल बनाने की अनुमति देती हैं, कुछ संसाधन प्रासंगिक और सटीक नहीं हो सकते हैं\nReference: आज ओईआर जिन चुनौतियों का सामना कर रहा है वे हैं गुणवत्ता के मुद्दे चूंकि कई ओईआर संग्राहक किसी भी उपयोगकर्ता को एक खाता खोलने और सामग्री पोस्ट करने की अनुमति देता है, तो कुछ संसाधन प्रासंगिक और एकदम सही नहीं हो सकते हैं\n------\nSource: If the problem it is silent, you can assume it but here it is specifically said that it is payable at the end of the year\nPredicted: यदि समस्या शान्त है, तो आप इसे मान सकते हैं, लेकिन यहाँ विशेष रूप से कहा गया है कि यह वर्ष के अंत में देय है\nReference: और विशेष रूप से जानकारी यहाँ दी गई है\n------\n\n\n\n--- Inference After Epoch 5.0 ---\nSource: Suppose there are two partners X and Y, out of 70 say it is 1:1\nPredicted: मान लीजिए कि दो साझेदार हैं X और Y, 70 में से यह 1:1 है\nReference: मान लीजिए कि दो साझेदार X और Y हैं, 70 में से यह 1: 1 है\n------\nSource: This stage uses information from the crime scene to determine both behavioral and personal characteristics and build an outline for the criminal profile\nPredicted: यह मंच अपराध स्थल से प्राप्त जानकारी का उपयोग व्यवहार संबंधी और व्यक्तिगत विशेषताओं को निर्धारित करने और आपराधिक रूपरेखा के लिए एक रूपरेखा बनाने के लिए करता है\nReference: यह चरण व्यवहार और व्यक्तिगत अभिलक्षणों दोनों को निर्धारित करने और आपराधिक प्रोफ़ाइल के लिए एक रूपरेखा बनाने के लिए अपराध स्थल से मिली जानकारी का उपयोग करता है\n------\nSource: They can assign the recovery material if necessary, define different paths for different learning goals\nPredicted: वे आवश्यक पर पुन:प्राप्ति सामग्री को सौंप सकते हैं, विभिन्न शिक्षण लक्ष्यों के लिए विभिन्न मार्गों को परिभाषित कर सकते हैं\nReference: यदि आवश्यक हो, तो वे प्राप्त की गयी सामग्री को असाइनमेंट के रूप में दे सकते हैं, विभिन्न शैक्षिक लक्ष्यों हेतु अलग अलग कार्यप्रणाली निर्धारित कर सकते हैं\n------\nSource: Next limitation is pedagogical constraints\nPredicted: अगली कमी शैक्षणिक बाधाएँ हैं\nReference: अगली कमी शैक्षणिक बाधाओं से संबंधित है\n------\nSource: The DMCA amended title number 17 of the US Code to extend the reach of copyright, while limiting the liability of the providers of online services for copyright infringement by its users\nPredicted: डीएमसीए ने कॉपीराइट के दायरे को बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया, इसके उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं की उत्तरदायित्व को सीमित किया\nReference: डीएमसीए ने अपने उपयोगकर्ताओं द्वारा कॉपीराइट उल्लंघन के लिए ऑनलाइन सेवाओं के प्रदाताओं के दायित्व को सीमित करते हुए कॉपीराइट की पहुंच बढ़ाने के लिए यूएस कोड की शीर्षक संख्या 17 में संशोधन किया\n------\nSource: In case of public companies having shareholder base at the date of the meeting more than 5,000 members then the quorum shall constitute 30 members personally present at the meeting\nPredicted: सार्वजनिक कंपनियों के मामले में, जिसकी बैठक की तारीख में शेयरहोल्डर के आधार पर 5,000 से अधिक सदस्य होते हैं, तो कोरम सदस्यों द्वारा बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन करेगा\nReference: सार्वजनिक कंपनियों के 5000 से अधिक सदस्यों की बैठक की तारीख में शेयरधारक, आधार होने की स्थिति में, कोरम बैठक में व्यक्तिगत रूप से उपस्थित 30 सदस्यों का गठन करेगा\n------\nSource: Any authentication method is selected based on how it is used, how it is implemented, it's effectiveness, whether the user accepts and user's attitude, how far it is secured, installation expenses, etc\nPredicted: किसी भी ऑथेंटिकेशन विधि को इस आधार पर चयनित किया जाता है कि कैसे इसका उपयोग किया जाता है, कैसे इसे लागू किया जाता है, इसकी प्रभावशीलता, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता का दृष्टिकोण, यह कितनी दूर सुरक्षित है, इंस्टॉलेशन खर्च आदि\nReference: किसी भी प्रमाणीकरण विधि का चयन इस आधार पर किया जाता है कि इसका उपयोग कैसे किया जाता है, इसे कैसे कार्यान्वित किया जाता है, इसकी प्रभावशीलता क्या है, क्या उपयोगकर्ता स्वीकार करता है और उपयोगकर्ता का दृष्टिकोण, यह कितना सुरक्षित है, स्थापना व्यय,आदि\n------\nSource: I can consider another important section that is section 35AD\nPredicted: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, वह धारा 35AD है\nReference: मैं एक अन्य महत्वपूर्ण धारा पर विचार कर सकता हूँ, जो कि धारा 35AD है\n------\nSource: The main challenges that an OER faces today are quality issues since many OER repositories allow any user to create an account and post material, some resources may not be relevant and accurate\nPredicted: आज एक ओईआर को जिन मुख्य चुनौतियों का सामना करना पड़ता है, वे गुणवत्ता के मुद्दे हैं, क्योंकि कई ओईआर रिपाज़िटोरी किसी भी यूजर को खाता बनाने और सामग्री पोस्ट करने की अनुमति देती हैं, कुछ संसाधन प्रासंगिक और सटीक नहीं हो सकते हैं\nReference: आज ओईआर जिन चुनौतियों का सामना कर रहा है वे हैं गुणवत्ता के मुद्दे चूंकि कई ओईआर संग्राहक किसी भी उपयोगकर्ता को एक खाता खोलने और सामग्री पोस्ट करने की अनुमति देता है, तो कुछ संसाधन प्रासंगिक और एकदम सही नहीं हो सकते हैं\n------\nSource: If the problem it is silent, you can assume it but here it is specifically said that it is payable at the end of the year\nPredicted: यदि समस्या शांत है, तो आप इसे मान सकते हैं, लेकिन यहाँ विशेष रूप से कहा गया है कि यह वर्ष के अंत में देय है\nReference: और विशेष रूप से जानकारी यहाँ दी गई है\n------\n\n\n\n✅ Inference samples saved to inference_samples.csv\n\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=11250, training_loss=0.18899773195054795, metrics={'train_runtime': 7768.6789, 'train_samples_per_second': 5.792, 'train_steps_per_second': 1.448, 'total_flos': 1.219010494464e+16, 'train_loss': 0.18899773195054795, 'epoch': 5.0})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}