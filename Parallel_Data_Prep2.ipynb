{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4b479e-cd82-4be7-8227-27e79ed0326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re # For parsing filenames if needed, though stem is often enough\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57444b9d-7ebb-43fe-a140-8a7ddbd1a0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set.\n"
     ]
    }
   ],
   "source": [
    "# --- Paths and Filenames ---\n",
    "CORPUS_CSV_PATH = \"text_corpus_10k_final.csv\" # Your input corpus CSV\n",
    "TEXTGRID_DIR_PATH = r\"C:\\Users\\vempa\\OneDrive\\Desktop\\ssmt_project_NEW\\aligned_output_10k_GT\\aligned_output\\speaker1\"          # Folder containing your TextGrid files\n",
    "SKIPPED_LOG_PATH = \"skipped_files.log\"       # Log file generated by the previous script\n",
    "OUTPUT_CLEANED_CSV_PATH = \"text_corpus_UGCE1_10k_cleaned.csv\" # Output for the cleaned corpus\n",
    "\n",
    "# --- Filename Pattern Configuration ---\n",
    "TEXTGRID_FILE_PREFIX = \"file_\"\n",
    "TEXTGRID_NUM_DIGITS = 6 # Number of digits in the filename (e.g., 000003)\n",
    "\n",
    "# --- Column Names ---\n",
    "INDEX_COLUMN = \"original_index\" # Name of the index column in your CSV\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49bbec46-a274-4566-bc40-212dc7ff9823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning TextGrid directory: C:\\Users\\vempa\\OneDrive\\Desktop\\ssmt_project_NEW\\aligned_output_10k_GT\\aligned_output\\speaker1...\n",
      "Found 9248 files matching the pattern in the directory.\n"
     ]
    }
   ],
   "source": [
    "textgrid_dir = Path(TEXTGRID_DIR_PATH)\n",
    "existing_basenames = set() # Use a set for fast lookups\n",
    "\n",
    "if not textgrid_dir.is_dir():\n",
    "    print(f\"Error: TextGrid directory not found at '{TEXTGRID_DIR_PATH}'. Please check the path.\")\n",
    "else:\n",
    "    print(f\"Scanning TextGrid directory: {textgrid_dir}...\")\n",
    "    # Regex to match the pattern and capture the base name (prefix + number)\n",
    "    # Allows for .textgrid or .TEXTGRID, case-insensitive extension\n",
    "    file_pattern = re.compile(rf\"^({TEXTGRID_FILE_PREFIX}\\d{{{TEXTGRID_NUM_DIGITS}}})\\.(TEXTGRID|textgrid)$\", re.IGNORECASE)\n",
    "\n",
    "    count = 0\n",
    "    for item in textgrid_dir.iterdir():\n",
    "        if item.is_file():\n",
    "             match = file_pattern.match(item.name)\n",
    "             if match:\n",
    "                base_name = match.group(1) # Get the 'file_XXXXXX' part\n",
    "                existing_basenames.add(base_name)\n",
    "                count += 1\n",
    "    print(f\"Found {count} files matching the pattern in the directory.\")\n",
    "    # print(f\"Sample existing basenames: {list(existing_basenames)[:5]}\") # Optional: print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a2e555-aecf-4f76-bb47-fd35de5f0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading skipped files log: skipped_files.log...\n",
      "Found 77 unique basenames in the skipped files log.\n"
     ]
    }
   ],
   "source": [
    "skipped_log_file = Path(SKIPPED_LOG_PATH)\n",
    "skipped_basenames = set() # Use a set\n",
    "\n",
    "if not skipped_log_file.is_file():\n",
    "    print(f\"Warning: Skipped files log not found at '{SKIPPED_LOG_PATH}'. Assuming no files were skipped.\")\n",
    "else:\n",
    "    print(f\"Reading skipped files log: {skipped_log_file}...\")\n",
    "    try:\n",
    "        with open(skipped_log_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                filename = line.strip()\n",
    "                if filename:\n",
    "                    # Extract basename from the logged filename (which might include extension)\n",
    "                    file_path = Path(filename)\n",
    "                    base_name = file_path.stem # .stem gets filename without extension\n",
    "                    # Optional: Validate format if needed, but usually stem is enough\n",
    "                    if base_name.startswith(TEXTGRID_FILE_PREFIX) and len(base_name) == len(TEXTGRID_FILE_PREFIX) + TEXTGRID_NUM_DIGITS:\n",
    "                         skipped_basenames.add(base_name)\n",
    "                    else:\n",
    "                         print(f\"  Warning: Skipping malformed line in log: {filename}\")\n",
    "        print(f\"Found {len(skipped_basenames)} unique basenames in the skipped files log.\")\n",
    "        # print(f\"Sample skipped basenames: {list(skipped_basenames)[:5]}\") # Optional: print sample\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading skipped files log '{SKIPPED_LOG_PATH}': {e}\")\n",
    "        print(\"Proceeding without skipped file information.\")\n",
    "        skipped_basenames = set() # Reset to empty set on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "928327ef-ebe7-4f97-944c-f02d523f384b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading corpus CSV: text_corpus_10k_final.csv...\n",
      "Loaded corpus with 10000 rows.\n",
      "Generating expected TextGrid basenames for filtering...\n",
      "Applying filters...\n",
      "\n",
      "Filtering results:\n",
      " - Original rows: 10000\n",
      " - Rows matching existing TextGrids: 9248\n",
      " - Rows matching non-skipped TextGrids: 9923\n",
      " - Rows kept after filtering (Exist AND Not Skipped): 9171\n",
      "\n",
      "Saving cleaned corpus to: text_corpus_UGCE1_10k_cleaned.csv...\n",
      "Cleaned corpus saved successfully.\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "corpus_file = Path(CORPUS_CSV_PATH)\n",
    "import csv\n",
    "if not corpus_file.is_file():\n",
    "    print(f\"Error: Corpus CSV file not found at '{CORPUS_CSV_PATH}'. Cannot proceed.\")\n",
    "else:\n",
    "    print(f\"\\nLoading corpus CSV: {corpus_file}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(corpus_file)\n",
    "        print(f\"Loaded corpus with {len(df)} rows.\")\n",
    "\n",
    "        if INDEX_COLUMN not in df.columns:\n",
    "            print(f\"Error: Index column '{INDEX_COLUMN}' not found in the CSV.\")\n",
    "            print(f\"Available columns are: {list(df.columns)}\")\n",
    "        else:\n",
    "            # --- Create the expected TextGrid basename for each row ---\n",
    "            print(\"Generating expected TextGrid basenames for filtering...\")\n",
    "            df['expected_basename'] = df[INDEX_COLUMN].apply(\n",
    "                lambda x: f\"{TEXTGRID_FILE_PREFIX}{x:0{TEXTGRID_NUM_DIGITS}d}\"\n",
    "            )\n",
    "\n",
    "            # --- Apply Filters ---\n",
    "            print(\"Applying filters...\")\n",
    "            # Condition 1: The expected basename MUST exist in the directory\n",
    "            mask_exists = df['expected_basename'].isin(existing_basenames)\n",
    "            # Condition 2: The expected basename must NOT be in the skipped list\n",
    "            mask_not_skipped = ~df['expected_basename'].isin(skipped_basenames) # Use ~ for NOT\n",
    "\n",
    "            # Combine masks: Row must satisfy BOTH conditions\n",
    "            final_mask = mask_exists & mask_not_skipped\n",
    "\n",
    "            # Apply the combined mask\n",
    "            df_cleaned = df[final_mask].copy()\n",
    "\n",
    "            # Remove the temporary column\n",
    "            df_cleaned = df_cleaned.drop(columns=['expected_basename'])\n",
    "\n",
    "            print(f\"\\nFiltering results:\")\n",
    "            print(f\" - Original rows: {len(df)}\")\n",
    "            print(f\" - Rows matching existing TextGrids: {mask_exists.sum()}\")\n",
    "            print(f\" - Rows matching non-skipped TextGrids: {mask_not_skipped.sum()}\")\n",
    "            print(f\" - Rows kept after filtering (Exist AND Not Skipped): {len(df_cleaned)}\")\n",
    "\n",
    "\n",
    "            # --- Save the cleaned DataFrame ---\n",
    "            output_file = Path(OUTPUT_CLEANED_CSV_PATH)\n",
    "            print(f\"\\nSaving cleaned corpus to: {output_file}...\")\n",
    "            # Use the same robust saving parameters as before\n",
    "            df_cleaned.to_csv(\n",
    "                output_file,\n",
    "                index=False,\n",
    "                encoding='utf-8',\n",
    "                quoting=csv.QUOTE_ALL,\n",
    "                doublequote=True,\n",
    "                escapechar='\\\\'\n",
    "            )\n",
    "            print(\"Cleaned corpus saved successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during CSV loading or processing: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c7c1a-ece2-4b08-a3eb-a3cab327d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
